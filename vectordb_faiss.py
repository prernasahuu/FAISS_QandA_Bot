# -*- coding: utf-8 -*-
"""VectorDB-faiss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDZVgvupxgVEU3qluzXLN12w_WinTeca
"""

#installing packages
!pip install langchain
!pip install faiss-cpu sentence-transformers

para= """A vector database is a specialized type of database designed to store and query high-dimensional vectors, which are numerical representations of data. These vectors typically result from machine learning models and are used to represent features of data such as images, text, audio, or other unstructured data types. The core purpose of a vector database is to enable efficient similarity search and retrieval of data based on proximity in vector space.
Key Features of Vector Databases:
High-Dimensional Vector Storage:
Unlike traditional databases that store structured data (e.g., tables with rows and columns), vector databases store embeddings or vectors, which are numerical arrays with many dimensions (often hundreds or thousands).
Similarity Search:
Vector databases are optimized for performing similarity searches. This involves finding vectors that are most similar to a given query vector based on distance metrics like:
Cosine Similarity
Dot Product
Euclidean Distance
The goal is to return vectors that represent data points closest to the query, which is useful for tasks like image search, document retrieval, and recommendation systems.
Efficient Querying:
Vector databases use techniques like Approximate Nearest Neighbor (ANN) algorithms (e.g., HNSW, IVF) to quickly retrieve the closest matches even when dealing with millions or billions of vectors.
Scalability:
Vector databases are designed to scale horizontally, meaning they can handle large volumes of data and queries efficiently, making them suitable for applications requiring real-time responses (e.g., search engines, recommendation systems).
Use Cases for Vector Databases:
Natural Language Processing (NLP): Storing and searching embeddings of text, enabling tasks like semantic search, document retrieval, and chatbots.
Image Search and Retrieval: Indexing images as vectors and performing image similarity search based on visual features.
Recommendation Engines: Storing user and item embeddings and finding the closest matches to recommend items based on user preferences.
Anomaly Detection: Detecting outliers or anomalies by measuring the distance of a new vector from known data vectors.
Examples of Vector Databases:
Pinecone: A fully managed vector database that allows storing, querying, and managing vector embeddings.
Milvus: An open-source vector database for embedding management and similarity search.
FAISS: A library by Facebook AI Research optimized for large-scale nearest neighbor search.
Weaviate: Another open-source vector database that supports semantic search and machine learning models.
How Vector Databases Differ from Traditional Databases:
Traditional databases (e.g., SQL databases) are optimized for querying structured data with exact matches (e.g., SELECT * FROM table WHERE id=123).
Vector databases are designed for approximate matches based on vector similarity, useful for unstructured data where "similarity" is more important than exactness."""
#splitting the paragraph via split (.)
sentences= para.split(',')
print(sentences)

#import libaries
import os
import faiss
from sentence_transformers import SentenceTransformer

from google.colab import userdata
HF_TOKEN=userdata.get('HF_TOKEN')

#initialising the miniLM version 6 word embedding model.(used hugging-face API)
model = SentenceTransformer('msmarco-MiniLM-L-6-v3')

#encoding the tokens/chunks with model.
embeddings= model.encode(sentences)
print(embeddings)

embeddings.shape

dimension = embeddings.shape[1] #as we are taking 1 sentence as a charateristic/deminsion.
print(dimension)
index = faiss.IndexFlatL2(dimension) #euclidean distance to vectorize the sentences according to similarities.
index.add(embeddings) #adding the embeddings into the faiss DB

"""#Direct approach to get efficient and accurate retrieval of answers based on the query asked.



"""

k=3 #top 3 results with respect to the query asked.
distances, indices = index.search(embeddings, k)

query= input("Ask the question: ")
query_emb= model.encode(query)
for i in range(k):
  print(f"{i+1}.{sentences[indices[0][i]]}(Distance: {distances[0][i]:.4f}")

"""#Retrival of the answer to the query, using gpt 3.5 turbo API.
##This provides an domain specific answer to the query.
"""

!pip install openai

from google.colab import userdata
API_KEY=userdata.get('API_KEY_MY')

import openai
from openai import OpenAI
os.environ["OPENAI_API_KEY"] = API_KEY
def generate_response(prompt):
    client = OpenAI()
    chat_completion = client.chat.completions.create(
    messages=[{"role": "system", "content": "You are a helpful assistant."},
     {"role": "user", "content": prompt}],
    model="gpt-3.5-turbo",
    )
    return chat_completion.choices[0].message.content

prefix="Answer the query specific to the given context."

prompt=f'''{prefix}\n\n\
Question: {query}\n\
Context: {sentences}\n\
Answer:'''

#GPT answer to the query.
print(generate_response(prompt))

!pip install gradio

"""#Interfacing Gradio with GPT 3.5 turbo."""

import gradio as gr
def chat_with_bot(query):
    return generate_response(query)

# Create the Gradio interface
iface = gr.Interface(
    fn=chat_with_bot,
    inputs=gr.Textbox(label="Enter the Query"),
    outputs=gr.Textbox(label="Response"),
    title="Q&A Bot",
    description="Ask anything and get response specific to the context,only."
)

# Launch the interface
iface.launch(debug=True)

